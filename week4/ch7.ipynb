{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.4 합성곱/풀링 계층 구현하기\n",
    "## 7.4.1 4차원 배열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.rand(10, 1, 28, 28)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 높이 28, 너비 28, 채널 1개인 데이터가 10개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#10개 중 첫 번째 데이터에 접근하려면 단순히 x[0]\n",
    "x[0].shape\n",
    "x[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12033843, 0.28919283, 0.37677769, 0.53403179, 0.27682319,\n",
       "        0.92986235, 0.45280887, 0.59880485, 0.37717265, 0.08944388,\n",
       "        0.23331252, 0.8953105 , 0.40133403, 0.84952056, 0.04897727,\n",
       "        0.8282982 , 0.97047449, 0.92421836, 0.84415563, 0.98114437,\n",
       "        0.34056012, 0.52226557, 0.75273761, 0.49064388, 0.29830609,\n",
       "        0.67498261, 0.04204519, 0.07455291],\n",
       "       [0.59148914, 0.13274061, 0.79931373, 0.57265647, 0.95643939,\n",
       "        0.91309698, 0.90525631, 0.71499649, 0.29335641, 0.3846483 ,\n",
       "        0.35634831, 0.43896115, 0.92100699, 0.03897701, 0.02141674,\n",
       "        0.03949298, 0.03878334, 0.57253885, 0.89886385, 0.27877892,\n",
       "        0.7044477 , 0.46871861, 0.5039713 , 0.90659491, 0.12310035,\n",
       "        0.70019986, 0.43823425, 0.70552543],\n",
       "       [0.67964373, 0.16782104, 0.87142695, 0.52901919, 0.45498174,\n",
       "        0.6760154 , 0.69039236, 0.09832803, 0.84457547, 0.93591812,\n",
       "        0.75311623, 0.40094423, 0.19163259, 0.37298266, 0.12007193,\n",
       "        0.81223551, 0.73980708, 0.78196068, 0.51301303, 0.26523153,\n",
       "        0.53446197, 0.35255257, 0.8038385 , 0.71449192, 0.85080015,\n",
       "        0.31636216, 0.98217182, 0.29585587],\n",
       "       [0.60871784, 0.61729621, 0.24647435, 0.24678053, 0.49993368,\n",
       "        0.83976015, 0.83601092, 0.25605192, 0.43793556, 0.8650734 ,\n",
       "        0.02340656, 0.14066381, 0.34532583, 0.12591227, 0.02214499,\n",
       "        0.68500618, 0.95539863, 0.93913098, 0.05026367, 0.30498846,\n",
       "        0.20405394, 0.7845405 , 0.77705463, 0.84624044, 0.04046737,\n",
       "        0.17176278, 0.14488748, 0.02267918],\n",
       "       [0.39349954, 0.92498092, 0.25674427, 0.60020528, 0.63404032,\n",
       "        0.38565298, 0.44592088, 0.87201869, 0.59573048, 0.66422506,\n",
       "        0.72172985, 0.05686934, 0.40979649, 0.80608256, 0.02608249,\n",
       "        0.8408502 , 0.32428836, 0.50360429, 0.67568666, 0.65462004,\n",
       "        0.69532727, 0.28170839, 0.89006889, 0.49874982, 0.92191596,\n",
       "        0.95523335, 0.09441783, 0.67000599],\n",
       "       [0.02255479, 0.99063145, 0.16655315, 0.38016715, 0.6609681 ,\n",
       "        0.96485394, 0.01977601, 0.66690013, 0.12154693, 0.12818391,\n",
       "        0.48105377, 0.7065564 , 0.45762499, 0.73484073, 0.29587152,\n",
       "        0.10357196, 0.86254348, 0.59183718, 0.36411311, 0.75348504,\n",
       "        0.16675711, 0.40818529, 0.95382037, 0.67655566, 0.71682754,\n",
       "        0.61951664, 0.57766708, 0.31630011],\n",
       "       [0.05118957, 0.78703875, 0.22866712, 0.25103521, 0.14230377,\n",
       "        0.92054285, 0.60282753, 0.16681532, 0.57567727, 0.97130015,\n",
       "        0.45889017, 0.62530598, 0.02369979, 0.04419339, 0.97435509,\n",
       "        0.54998609, 0.7060361 , 0.51708908, 0.9127814 , 0.61538275,\n",
       "        0.36980888, 0.73825459, 0.08096965, 0.39472663, 0.80765157,\n",
       "        0.07398784, 0.55690522, 0.77367113],\n",
       "       [0.18778747, 0.36405813, 0.91301158, 0.16061457, 0.96916365,\n",
       "        0.37884563, 0.75636452, 0.66052232, 0.56267974, 0.7869868 ,\n",
       "        0.64600674, 0.01900582, 0.53275546, 0.63510574, 0.89221888,\n",
       "        0.89518207, 0.65930482, 0.73533252, 0.1794611 , 0.96718868,\n",
       "        0.23570398, 0.65582719, 0.32375013, 0.63471663, 0.23877879,\n",
       "        0.77129985, 0.22373636, 0.57641317],\n",
       "       [0.49284559, 0.13132436, 0.08733997, 0.27367936, 0.48842862,\n",
       "        0.43748305, 0.57231701, 0.47787416, 0.69457822, 0.39879739,\n",
       "        0.88008067, 0.39284949, 0.06545395, 0.57156978, 0.89681653,\n",
       "        0.94443852, 0.61812661, 0.83619574, 0.4147966 , 0.0458856 ,\n",
       "        0.08521964, 0.11545265, 0.872564  , 0.81320586, 0.55602992,\n",
       "        0.05586469, 0.37011533, 0.52213526],\n",
       "       [0.04893608, 0.06293058, 0.37881235, 0.01039951, 0.31151277,\n",
       "        0.92356866, 0.66298572, 0.97555784, 0.57959415, 0.56333251,\n",
       "        0.83351521, 0.37979573, 0.06150167, 0.47312781, 0.47671682,\n",
       "        0.79658093, 0.93230835, 0.59336602, 0.26944468, 0.05776228,\n",
       "        0.43932725, 0.41652965, 0.12693894, 0.94307322, 0.87183586,\n",
       "        0.57357292, 0.80106988, 0.37659355],\n",
       "       [0.98846562, 0.37755499, 0.4880489 , 0.69837629, 0.54149539,\n",
       "        0.91722855, 0.23826895, 0.1959697 , 0.93729916, 0.19792249,\n",
       "        0.09249667, 0.55935252, 0.50415273, 0.95778228, 0.87978879,\n",
       "        0.40007046, 0.39463703, 0.70623104, 0.56229331, 0.2051446 ,\n",
       "        0.13209167, 0.00573601, 0.80555099, 0.45910461, 0.3446947 ,\n",
       "        0.74889929, 0.83157482, 0.21409102],\n",
       "       [0.14353281, 0.45748065, 0.97275048, 0.40979864, 0.74484741,\n",
       "        0.06392987, 0.37007142, 0.35237898, 0.72258623, 0.58635605,\n",
       "        0.7854724 , 0.54980268, 0.52574452, 0.80325008, 0.11028171,\n",
       "        0.03856339, 0.98372221, 0.41208512, 0.29738031, 0.38961015,\n",
       "        0.66075263, 0.30065955, 0.93511362, 0.79820371, 0.62992972,\n",
       "        0.28384161, 0.17775663, 0.84031176],\n",
       "       [0.13700742, 0.27919452, 0.42442659, 0.72540086, 0.67792105,\n",
       "        0.85490979, 0.557742  , 0.72826064, 0.29430004, 0.80486288,\n",
       "        0.75483522, 0.05034237, 0.14635553, 0.22023108, 0.57345662,\n",
       "        0.45153596, 0.33514388, 0.72288713, 0.05032946, 0.53092296,\n",
       "        0.96163128, 0.99780631, 0.50070565, 0.22296517, 0.76189124,\n",
       "        0.20836069, 0.73672891, 0.44859826],\n",
       "       [0.67467673, 0.02494751, 0.51046208, 0.67525231, 0.28850895,\n",
       "        0.21600902, 0.15722913, 0.26984124, 0.5748518 , 0.13307043,\n",
       "        0.41887299, 0.88382703, 0.02506217, 0.35944682, 0.71824638,\n",
       "        0.94886679, 0.82761065, 0.45841239, 0.26962462, 0.53314252,\n",
       "        0.44524777, 0.14000962, 0.57434611, 0.32872072, 0.97767335,\n",
       "        0.71308617, 0.58140123, 0.48266807],\n",
       "       [0.89538239, 0.76346171, 0.08309077, 0.90392486, 0.63150782,\n",
       "        0.20612005, 0.28295486, 0.09897144, 0.67477956, 0.3327159 ,\n",
       "        0.45610127, 0.41973533, 0.01914531, 0.2895421 , 0.1873225 ,\n",
       "        0.4347916 , 0.51066439, 0.3834733 , 0.2596675 , 0.34319225,\n",
       "        0.83887188, 0.65286141, 0.4005876 , 0.03763006, 0.6496045 ,\n",
       "        0.29097417, 0.43275666, 0.95402408],\n",
       "       [0.6849766 , 0.66295196, 0.50278724, 0.95626261, 0.19395422,\n",
       "        0.46068121, 0.92086152, 0.36290523, 0.78400779, 0.70353508,\n",
       "        0.21352873, 0.16971007, 0.44364569, 0.19439307, 0.85945388,\n",
       "        0.0168496 , 0.48685152, 0.71124127, 0.93771171, 0.37191944,\n",
       "        0.1333119 , 0.74142462, 0.40871597, 0.71051048, 0.5329283 ,\n",
       "        0.75673911, 0.61989704, 0.69585537],\n",
       "       [0.83594638, 0.59453889, 0.07338863, 0.25526087, 0.37024971,\n",
       "        0.30141618, 0.40444574, 0.01393938, 0.49037084, 0.70254275,\n",
       "        0.68125881, 0.3099338 , 0.40103926, 0.5971476 , 0.50138093,\n",
       "        0.44933898, 0.61595172, 0.77218874, 0.37944435, 0.44726779,\n",
       "        0.68829353, 0.98597509, 0.69530069, 0.16177399, 0.95747024,\n",
       "        0.16989327, 0.93175811, 0.29149232],\n",
       "       [0.30233829, 0.12248443, 0.76862551, 0.73232852, 0.45761274,\n",
       "        0.37809157, 0.69240348, 0.0863191 , 0.99548407, 0.44125377,\n",
       "        0.93967266, 0.30387998, 0.19026064, 0.1910686 , 0.13822448,\n",
       "        0.50117762, 0.23997663, 0.83289319, 0.48620256, 0.39741991,\n",
       "        0.97542909, 0.83184023, 0.05981714, 0.08548251, 0.57196948,\n",
       "        0.11289832, 0.99453938, 0.36681825],\n",
       "       [0.22496225, 0.88929712, 0.90503245, 0.66912715, 0.46248475,\n",
       "        0.56598293, 0.25497135, 0.51661775, 0.65352161, 0.77472985,\n",
       "        0.49518338, 0.68675853, 0.60235967, 0.51846429, 0.1345588 ,\n",
       "        0.05788457, 0.13447623, 0.49419291, 0.47671818, 0.50263964,\n",
       "        0.00891386, 0.29598044, 0.3587685 , 0.86144221, 0.08648967,\n",
       "        0.39821311, 0.34194148, 0.10334579],\n",
       "       [0.16660297, 0.17283682, 0.89294754, 0.81573457, 0.89411092,\n",
       "        0.33449522, 0.27000915, 0.95516923, 0.72578869, 0.86076419,\n",
       "        0.12276358, 0.08874893, 0.6065497 , 0.01610624, 0.20349217,\n",
       "        0.23242851, 0.38713041, 0.32023959, 0.80152971, 0.80711856,\n",
       "        0.1346049 , 0.25627606, 0.61673053, 0.90923168, 0.14979648,\n",
       "        0.35451942, 0.02916582, 0.61984579],\n",
       "       [0.5193519 , 0.9559872 , 0.03377083, 0.71658454, 0.77977904,\n",
       "        0.53404566, 0.71172192, 0.35278824, 0.26158219, 0.60462662,\n",
       "        0.61702064, 0.0893305 , 0.44569286, 0.99160524, 0.12415595,\n",
       "        0.41289537, 0.13106495, 0.50022768, 0.88638314, 0.71414755,\n",
       "        0.90110876, 0.49460012, 0.21599879, 0.49480944, 0.97719586,\n",
       "        0.91694652, 0.68208762, 0.64174493],\n",
       "       [0.65287944, 0.55977758, 0.52434529, 0.27728149, 0.45027085,\n",
       "        0.99880176, 0.85422579, 0.44334794, 0.0359165 , 0.52714946,\n",
       "        0.19303255, 0.64384347, 0.81511484, 0.64875194, 0.74309492,\n",
       "        0.99100796, 0.40650187, 0.92756021, 0.41153377, 0.99654684,\n",
       "        0.92006173, 0.98143762, 0.62406789, 0.93879658, 0.31925775,\n",
       "        0.73406445, 0.54811922, 0.77936391],\n",
       "       [0.1864238 , 0.47136133, 0.53901028, 0.54914826, 0.00894166,\n",
       "        0.65972515, 0.74212063, 0.74545144, 0.44697803, 0.27560471,\n",
       "        0.75546814, 0.67078016, 0.86573984, 0.15134431, 0.27264529,\n",
       "        0.26405265, 0.7924156 , 0.03713744, 0.49157839, 0.59655487,\n",
       "        0.47293617, 0.07937569, 0.95488864, 0.52972124, 0.85456857,\n",
       "        0.53560573, 0.55549216, 0.82630959],\n",
       "       [0.80215344, 0.83336268, 0.51177655, 0.58165439, 0.9672745 ,\n",
       "        0.73697611, 0.44300365, 0.37718311, 0.89345643, 0.60431963,\n",
       "        0.77472041, 0.62724987, 0.96545481, 0.612893  , 0.09374246,\n",
       "        0.79156485, 0.84114903, 0.13425761, 0.49405201, 0.77416537,\n",
       "        0.20061987, 0.82004017, 0.67268338, 0.25151919, 0.7074661 ,\n",
       "        0.32238734, 0.94986427, 0.37391907],\n",
       "       [0.47783921, 0.18016287, 0.94968702, 0.2229471 , 0.94023808,\n",
       "        0.99671445, 0.18485796, 0.23964884, 0.58756513, 0.8974809 ,\n",
       "        0.95249332, 0.39032375, 0.04923465, 0.26532218, 0.00549841,\n",
       "        0.27404555, 0.28330652, 0.43844841, 0.7315692 , 0.954108  ,\n",
       "        0.11715955, 0.61896059, 0.35106336, 0.84755908, 0.43889249,\n",
       "        0.15323454, 0.89782713, 0.06676741],\n",
       "       [0.14455452, 0.18510939, 0.4889347 , 0.35912715, 0.06985824,\n",
       "        0.78019424, 0.95473797, 0.89026653, 0.63739602, 0.14793027,\n",
       "        0.77269684, 0.11295052, 0.11297563, 0.83359606, 0.72314904,\n",
       "        0.941989  , 0.03894196, 0.54051099, 0.2257406 , 0.90565827,\n",
       "        0.77163339, 0.67715803, 0.24413646, 0.03675632, 0.98374278,\n",
       "        0.48718981, 0.90534496, 0.30246898],\n",
       "       [0.69187034, 0.99560058, 0.17933687, 0.14182999, 0.39595403,\n",
       "        0.81859916, 0.23310273, 0.11880427, 0.39694245, 0.79914873,\n",
       "        0.66534181, 0.8987918 , 0.79035066, 0.34036151, 0.93120642,\n",
       "        0.55989168, 0.23473421, 0.03250321, 0.97943975, 0.47759386,\n",
       "        0.43565978, 0.17990401, 0.85730115, 0.91303489, 0.92157242,\n",
       "        0.9787074 , 0.51285058, 0.62229493],\n",
       "       [0.1138431 , 0.78630352, 0.86691983, 0.24733466, 0.93941843,\n",
       "        0.15985694, 0.22541025, 0.27264097, 0.06631831, 0.40711737,\n",
       "        0.28430446, 0.88446519, 0.34390719, 0.92740694, 0.69243569,\n",
       "        0.13703624, 0.94888334, 0.29129968, 0.12668323, 0.19158207,\n",
       "        0.12762155, 0.73274318, 0.04188323, 0.65687434, 0.77474756,\n",
       "        0.32359398, 0.53962952, 0.88440487]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#첫 번째 데이터의 첫 채널의 공간 데이터에 접근\n",
    "x[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4.2 im2col로 데이터 전개하기\n",
    "#### im2col은 입력 데이터를 필터링(가중치 계산)하기 좋게 전개하는(펼치는) 함수\n",
    "#### 3차원 입력 데이터에 im2col을 적용하면 2차원 행렬로 바뀜(4차원->2차원)\n",
    "<img src=\"refimg/7-18.png\">\n",
    "\n",
    "### 합성 계층의 구현 흐름\n",
    "<img src=\"refimg/7-19.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#im2col 함수의 구현 및 인터페이스 설명\n",
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    col : 2차원 배열\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant') #합성곱에서 padding 구현\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* input_data - (데이터 수, 채널 수, 높이, 너비)의 4차원 배열로 이뤄진 입력 데이터\n",
    "* filter_h - 필터의 높이\n",
    "* filter_w - 필터의 너비\n",
    "* stride - 스트라이드\n",
    "* pad - 패딩\n",
    "\n",
    "\n",
    "이 im2col은 '필터 크기', '스트라이드', '패딩'을 고려하여 입력 데이터를 2차원 배열로 전개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "x1 = np.random.rand(1, 3, 7, 7)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape)\n",
    "\n",
    "x2 = np.random.rand(10, 3, 7, 7)\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4.3 합성곱 계층 구현하기\n",
    "[심화 과정](\"https://www.slideshare.net/leeseungeun/cnn-vgg-72164295\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "합성곱 계층을 구현. forward와 backward 메서드를 활용. \n",
    "역전파를 구현하기 위해 im2col을 역전해 col2im을 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"(im2col과 반대) 2차원 배열을 입력받아 다수의 이미지 묶음으로 변환한다.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    col : 2차원 배열(입력 데이터)\n",
    "    input_shape : 원래 이미지 데이터의 형상（예：(10, 1, 28, 28)）\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    img : 변환된 이미지들\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        # 중간 데이터（backward 시 사용）\n",
    "        self.x = None   \n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        # 가중치와 편향 매개변수의 기울기\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        #필터(가중치)는 (FN, C, FH, FW)의 4차원 형상\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "        \n",
    "        #이 부분 중요-> 입력 데이터를 im2col로 전개하고 필터도 reshape를 활용해 2차원 배열로 전개\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T #필터 전개\n",
    "        out = np.dot(col, col_W) + self.b #행렬 곱\n",
    "        \n",
    "        #-1, transpose -> 출력 데이터를 적절한 형상으로 바꿔줌(p247)\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout): #2차원 -> 4차원\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "        \n",
    "        #편향의 첫 번째 축의 합-합 노드이므로 별도 수식 없이 그대로\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout) #가중치 전치 행렬과의 행렬곱\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"refimg/5-27.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4.4 풀링 계층 구현하기\n",
    "### 채널 쪽이 독립적이라는 점이 합성곱 계층 때와 다름\n",
    "#### 입력 데이터를 전개한 후, 전개한 행렬에서 행 별 최댓값을 구하고 적절한 형상으로 성형\n",
    "<img src=\"refimg/7-22.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        #전개(1)\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "\n",
    "        #최댓값(2)\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1) #2차원 배열, 행렬이라면 axis=0은 열 방향, 1은 행 방향\n",
    "        #성형(3)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    #5.5.1 ReLU의 max 역전파 참고\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.5 CNN 구현하기\n",
    "simpleConvNet 클래스 \n",
    "초기화 때 받는 인수\n",
    "* input_dim – 입력 데이터(채널 수, 높이, 너비)의 차원\n",
    "* conv_param – 합성곱 계층의 하이퍼파라미터(딕셔너리), 딕셔너리의 키는 다음과 같다.\n",
    "  - filter_num – 필터 수\n",
    "  - filter_size – 필터 크기\n",
    "  - stride – 스트라이드\n",
    "  - pad – 패딩\n",
    "* hidden_size – 은닉층(완전연결)의 뉴런 수\n",
    "* output_size – 출력층(완전연결)의 뉴런 수\n",
    "* weight_init_std – 초기화 때의 가중치 표준편차\n",
    "\n",
    "\n",
    "합성곱 계층의 하이퍼 파라미터는 딕셔너리 형태로 주어지며, {‘filter_num’:30, ‘filter_size’:5, ‘pad’:0, ‘stride’:1}처럼 저장된다는 뜻.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"단순한 합성곱 신경망\n",
    "    \n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
    "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
    "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
    "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
    "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
    "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
    "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        #초기화 인수로 주어진 합성곱 계층의 하이퍼 파라미터를 딕셔너리에서 꺼냄\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        #합성곱 계층의 출력 크기를 계산\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 가중치 초기화 - 매개 변수들을 params 딕셔너리에 저장\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성 - 순서가 있는 딕셔너리인 layers에 계층들 차례로 추가\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        #별도 변수에 저장\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    #초기화 때 layers에 추가한 계층을 맨 앞에서부터 \n",
    "    #차례로 forward 메서드를 호출하여 그 결과를 다음 계층에 전달\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    #predict 메서드의 결과를 인수로 마지막 층의 forward 메서드를 호출\n",
    "    #즉, 첫 계층부터 마지막 계층까지 forward를 처리함\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실 함수를 구한다.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다（수치미분）.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    #매개변수의 기울기 구하기 - 순전파와 역전파를 반복\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다(오차역전파법).\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2995086500161093\n",
      "=== epoch:1, train acc:0.198, test acc:0.199 ===\n",
      "train loss:2.299309931107939\n",
      "train loss:2.293338804427441\n",
      "train loss:2.287625785286728\n",
      "train loss:2.2818144246885184\n",
      "train loss:2.269634842833055\n",
      "train loss:2.254377864588165\n",
      "train loss:2.226564821985859\n",
      "train loss:2.2316145422583786\n",
      "train loss:2.1855816356859\n",
      "train loss:2.1584000361000473\n",
      "train loss:2.0862288803544957\n",
      "train loss:2.059547388936664\n",
      "train loss:2.010987297135968\n",
      "train loss:1.9249063315018093\n",
      "train loss:1.9183326075620757\n",
      "train loss:1.8149587793028312\n",
      "train loss:1.83834962630422\n",
      "train loss:1.732641302545534\n",
      "train loss:1.5414873734999568\n",
      "train loss:1.4715661622789111\n",
      "train loss:1.5532683413043678\n",
      "train loss:1.4858979225854387\n",
      "train loss:1.1535773907581754\n",
      "train loss:1.2591809795126254\n",
      "train loss:1.1193526286338615\n",
      "train loss:1.080071590911592\n",
      "train loss:1.0257170770216169\n",
      "train loss:0.9630280280512636\n",
      "train loss:0.8695949125921494\n",
      "train loss:0.8564591932700933\n",
      "train loss:0.8944338063660446\n",
      "train loss:0.8463357270637941\n",
      "train loss:0.6955773803086303\n",
      "train loss:0.6731803217963135\n",
      "train loss:0.6986128427061178\n",
      "train loss:0.6441929288824944\n",
      "train loss:0.6765045071757706\n",
      "train loss:0.7349767012555409\n",
      "train loss:0.6000007199172762\n",
      "train loss:0.7334993565361836\n",
      "train loss:0.683965143236789\n",
      "train loss:0.7900480462709237\n",
      "train loss:0.6334244679644333\n",
      "train loss:0.4561042677745831\n",
      "train loss:0.4836875358658495\n",
      "train loss:0.43814722560821073\n",
      "train loss:0.6273077603832875\n",
      "train loss:0.568233507113251\n",
      "train loss:0.5150903049499912\n",
      "train loss:0.6927526770643821\n",
      "=== epoch:2, train acc:0.813, test acc:0.793 ===\n",
      "train loss:0.45988953010965283\n",
      "train loss:0.45327904909074024\n",
      "train loss:0.3383488646241593\n",
      "train loss:0.4208874464704219\n",
      "train loss:0.6304220452510276\n",
      "train loss:0.43188462926680105\n",
      "train loss:0.42663704990672163\n",
      "train loss:0.7204144162163979\n",
      "train loss:0.47074071094231684\n",
      "train loss:0.4761904970520731\n",
      "train loss:0.34334323496144825\n",
      "train loss:0.3083475080129932\n",
      "train loss:0.49663330274379774\n",
      "train loss:0.4423885701086539\n",
      "train loss:0.5033646949533191\n",
      "train loss:0.5526869917002767\n",
      "train loss:0.4352238379309401\n",
      "train loss:0.33555301797396564\n",
      "train loss:0.48723529200169885\n",
      "train loss:0.5077448096677315\n",
      "train loss:0.4416315868324434\n",
      "train loss:0.4504251726420309\n",
      "train loss:0.46001871679494216\n",
      "train loss:0.4226530405455646\n",
      "train loss:0.3260571499095034\n",
      "train loss:0.3661524214206775\n",
      "train loss:0.4499887260036295\n",
      "train loss:0.40107266817205306\n",
      "train loss:0.5515813818100184\n",
      "train loss:0.34821897847660355\n",
      "train loss:0.42504590024943106\n",
      "train loss:0.34011362312011717\n",
      "train loss:0.36861458993895757\n",
      "train loss:0.39923219994790016\n",
      "train loss:0.6057562611745503\n",
      "train loss:0.38810077129604204\n",
      "train loss:0.47438922447284426\n",
      "train loss:0.30452375566714357\n",
      "train loss:0.2953118054687002\n",
      "train loss:0.40189999298311846\n",
      "train loss:0.2535996177568485\n",
      "train loss:0.5298491231116222\n",
      "train loss:0.4455067987628107\n",
      "train loss:0.37222767234504145\n",
      "train loss:0.36447431450641327\n",
      "train loss:0.29324645656899656\n",
      "train loss:0.3078551728932019\n",
      "train loss:0.5602806563359338\n",
      "train loss:0.25427823357979945\n",
      "train loss:0.33883668116509447\n",
      "=== epoch:3, train acc:0.877, test acc:0.875 ===\n",
      "train loss:0.30678754819269693\n",
      "train loss:0.44772036324640857\n",
      "train loss:0.4143400996056103\n",
      "train loss:0.2899793026868583\n",
      "train loss:0.43598340283503245\n",
      "train loss:0.3535836656234158\n",
      "train loss:0.3594133521115825\n",
      "train loss:0.3604400708778634\n",
      "train loss:0.22046105677217212\n",
      "train loss:0.2694585315242539\n",
      "train loss:0.2896429640649514\n",
      "train loss:0.3705015834991323\n",
      "train loss:0.1929915393283416\n",
      "train loss:0.3249763477760215\n",
      "train loss:0.3408466744819655\n",
      "train loss:0.2851484537253129\n",
      "train loss:0.2912338995481257\n",
      "train loss:0.4683630874183215\n",
      "train loss:0.2888531665900309\n",
      "train loss:0.2987957479817287\n",
      "train loss:0.3688779890941776\n",
      "train loss:0.25747476759778926\n",
      "train loss:0.284170199166055\n",
      "train loss:0.31102430536144293\n",
      "train loss:0.31221308573711276\n",
      "train loss:0.3350267480438993\n",
      "train loss:0.30966270558604597\n",
      "train loss:0.34229906103162056\n",
      "train loss:0.25752603292500004\n",
      "train loss:0.23294940112368726\n",
      "train loss:0.35842690881444983\n",
      "train loss:0.2855911822139278\n",
      "train loss:0.29347453858587397\n",
      "train loss:0.268671796415722\n",
      "train loss:0.4044517158624079\n",
      "train loss:0.3501648621785394\n",
      "train loss:0.32671931157781325\n",
      "train loss:0.16014193529649415\n",
      "train loss:0.36511857229267486\n",
      "train loss:0.2982896379535945\n",
      "train loss:0.2111529298914791\n",
      "train loss:0.34294739166288074\n",
      "train loss:0.39955537168494387\n",
      "train loss:0.4231583862550983\n",
      "train loss:0.3215590399432516\n",
      "train loss:0.2783339248138643\n",
      "train loss:0.30608415872145256\n",
      "train loss:0.2960475822640852\n",
      "train loss:0.32205746432317617\n",
      "train loss:0.2383328244976629\n",
      "=== epoch:4, train acc:0.89, test acc:0.896 ===\n",
      "train loss:0.4119164470396961\n",
      "train loss:0.3136673422391461\n",
      "train loss:0.3563676303473489\n",
      "train loss:0.2473376128029502\n",
      "train loss:0.2389390308838056\n",
      "train loss:0.35112022047348235\n",
      "train loss:0.3604940800739195\n",
      "train loss:0.2083589104465137\n",
      "train loss:0.1991863288835335\n",
      "train loss:0.26792474498085933\n",
      "train loss:0.22469451324566803\n",
      "train loss:0.25134598210064263\n",
      "train loss:0.37887832754126094\n",
      "train loss:0.2715837066428371\n",
      "train loss:0.1824011185999046\n",
      "train loss:0.18721092705540898\n",
      "train loss:0.18295017690063267\n",
      "train loss:0.1804715449763756\n",
      "train loss:0.16992628342641447\n",
      "train loss:0.43524615503962993\n",
      "train loss:0.21928724749671172\n",
      "train loss:0.40191925542350276\n",
      "train loss:0.28552096457771503\n",
      "train loss:0.41512118828470435\n",
      "train loss:0.20112502636766602\n",
      "train loss:0.35999714866468724\n",
      "train loss:0.32348292068042006\n",
      "train loss:0.400455938275157\n",
      "train loss:0.27930947555847985\n",
      "train loss:0.17261835958584645\n",
      "train loss:0.15603032779366433\n",
      "train loss:0.1571944433258719\n",
      "train loss:0.20183170879129775\n",
      "train loss:0.18974128399915585\n",
      "train loss:0.2024231773360084\n",
      "train loss:0.1423850236482789\n",
      "train loss:0.14974143393563275\n",
      "train loss:0.27555244299379944\n",
      "train loss:0.2610462366176934\n",
      "train loss:0.28210808402457366\n",
      "train loss:0.23158882510784767\n",
      "train loss:0.1399958153503144\n",
      "train loss:0.14512910665173748\n",
      "train loss:0.20571097628274487\n",
      "train loss:0.2425020174149728\n",
      "train loss:0.20526183981869475\n",
      "train loss:0.17053101699678236\n",
      "train loss:0.21675088713145965\n",
      "train loss:0.2549370743814624\n",
      "train loss:0.14542866919117706\n",
      "=== epoch:5, train acc:0.912, test acc:0.897 ===\n",
      "train loss:0.13485533772852054\n",
      "train loss:0.19671396513546022\n",
      "train loss:0.16345396723019157\n",
      "train loss:0.22423509473206166\n",
      "train loss:0.22430050923469036\n",
      "train loss:0.2811885458111218\n",
      "train loss:0.10420506464727199\n",
      "train loss:0.24640013340639544\n",
      "train loss:0.15027822004716745\n",
      "train loss:0.173823383046209\n",
      "train loss:0.1652927807008787\n",
      "train loss:0.30042585657569015\n",
      "train loss:0.20747230250943433\n",
      "train loss:0.18324456871263675\n",
      "train loss:0.10543482067015296\n",
      "train loss:0.1527506675816034\n",
      "train loss:0.22527284693612135\n",
      "train loss:0.1866098255498746\n",
      "train loss:0.15400569251894353\n",
      "train loss:0.16884806175162004\n",
      "train loss:0.14129344763216378\n",
      "train loss:0.13827408775658387\n",
      "train loss:0.23098564920174358\n",
      "train loss:0.2439760242226641\n",
      "train loss:0.4554863543528297\n",
      "train loss:0.34141306498582336\n",
      "train loss:0.14983739359044393\n",
      "train loss:0.19684025486989076\n",
      "train loss:0.25244639213596914\n",
      "train loss:0.13351994928942676\n",
      "train loss:0.10781411154868312\n",
      "train loss:0.2698096718363007\n",
      "train loss:0.3098776234994241\n",
      "train loss:0.11164845552218877\n",
      "train loss:0.2558463658837045\n",
      "train loss:0.23811641598191027\n",
      "train loss:0.1870435945239253\n",
      "train loss:0.2672585313568575\n",
      "train loss:0.12740297824766975\n",
      "train loss:0.09791863640531316\n",
      "train loss:0.23345129568725612\n",
      "train loss:0.26115787132153767\n",
      "train loss:0.3056878569389862\n",
      "train loss:0.2511920848839001\n",
      "train loss:0.16604099923616908\n",
      "train loss:0.20315741158377296\n",
      "train loss:0.20416642429422546\n",
      "train loss:0.21351847107287864\n",
      "train loss:0.24404773832940058\n",
      "train loss:0.144542671212821\n",
      "=== epoch:6, train acc:0.918, test acc:0.92 ===\n",
      "train loss:0.20146237888856175\n",
      "train loss:0.1874182475321332\n",
      "train loss:0.11357822877627897\n",
      "train loss:0.21213235422642324\n",
      "train loss:0.16215019191598373\n",
      "train loss:0.17294578639733968\n",
      "train loss:0.1880591738931502\n",
      "train loss:0.13206858947162745\n",
      "train loss:0.1814662163692122\n",
      "train loss:0.294310119693469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.12102166411282442\n",
      "train loss:0.22956785130243207\n",
      "train loss:0.2866611760737355\n",
      "train loss:0.15937152071805305\n",
      "train loss:0.20208747754680356\n",
      "train loss:0.4107326189288824\n",
      "train loss:0.14126361226801826\n",
      "train loss:0.165670959869273\n",
      "train loss:0.10417358913809775\n",
      "train loss:0.3133895945918418\n",
      "train loss:0.33415087863227455\n",
      "train loss:0.1875811711713524\n",
      "train loss:0.17016436469596932\n",
      "train loss:0.18155991794748785\n",
      "train loss:0.3134717308954751\n",
      "train loss:0.16372452324726924\n",
      "train loss:0.12041949441419612\n",
      "train loss:0.23499289546575658\n",
      "train loss:0.07557453790989259\n",
      "train loss:0.23820903692081097\n",
      "train loss:0.23129361618210198\n",
      "train loss:0.16625906998119894\n",
      "train loss:0.2606536187856524\n",
      "train loss:0.15593979618106035\n",
      "train loss:0.09211725949113408\n",
      "train loss:0.1926515975066704\n",
      "train loss:0.12003242650615713\n",
      "train loss:0.1339175402981548\n",
      "train loss:0.13126725824514512\n",
      "train loss:0.20047131196081047\n",
      "train loss:0.09022234333916336\n",
      "train loss:0.2226482593276533\n",
      "train loss:0.12286715446080454\n",
      "train loss:0.1801833919320198\n",
      "train loss:0.10861427196025007\n",
      "train loss:0.172849460361305\n",
      "train loss:0.1195779147085419\n",
      "train loss:0.1316645674564998\n",
      "train loss:0.14448781091135024\n",
      "train loss:0.21560298798632135\n",
      "=== epoch:7, train acc:0.942, test acc:0.921 ===\n",
      "train loss:0.19408224935744006\n",
      "train loss:0.15880542617743676\n",
      "train loss:0.15345401969781503\n",
      "train loss:0.17574066354735648\n",
      "train loss:0.1729969917554881\n",
      "train loss:0.15395453742775653\n",
      "train loss:0.18931973514516623\n",
      "train loss:0.13584156853880486\n",
      "train loss:0.15390455290207697\n",
      "train loss:0.14294481842919807\n",
      "train loss:0.14287722459557206\n",
      "train loss:0.11834363967217765\n",
      "train loss:0.10913741719635302\n",
      "train loss:0.20700374134363284\n",
      "train loss:0.11277323691326811\n",
      "train loss:0.17221243293874\n",
      "train loss:0.21422547483134377\n",
      "train loss:0.1711191789021009\n",
      "train loss:0.10412544709864634\n",
      "train loss:0.18005420787014872\n",
      "train loss:0.14385691758303232\n",
      "train loss:0.17663743867756815\n",
      "train loss:0.19853144419232543\n",
      "train loss:0.14289910902315914\n",
      "train loss:0.24150800438684938\n",
      "train loss:0.11387320252657596\n",
      "train loss:0.07504674538815609\n",
      "train loss:0.15753968839934168\n",
      "train loss:0.1755189945114207\n",
      "train loss:0.14821106201719192\n",
      "train loss:0.12747649206817663\n",
      "train loss:0.1146369494729735\n",
      "train loss:0.12144888721648789\n",
      "train loss:0.14661513184719058\n",
      "train loss:0.09666847332278651\n",
      "train loss:0.14511699684000143\n",
      "train loss:0.45108345318530424\n",
      "train loss:0.11936164173615946\n",
      "train loss:0.08863374055259872\n",
      "train loss:0.06179361408027666\n",
      "train loss:0.09356766167272007\n",
      "train loss:0.11978357506578648\n",
      "train loss:0.2172973310669069\n",
      "train loss:0.08848345202974742\n",
      "train loss:0.12173874549571467\n",
      "train loss:0.12445144474262346\n",
      "train loss:0.336141044027406\n",
      "train loss:0.16862617078408984\n",
      "train loss:0.11565395906060238\n",
      "train loss:0.23861806077011707\n",
      "=== epoch:8, train acc:0.952, test acc:0.925 ===\n",
      "train loss:0.07419421626204724\n",
      "train loss:0.21537901285442726\n",
      "train loss:0.05813211547559646\n",
      "train loss:0.09880639272041572\n",
      "train loss:0.1774092341513115\n",
      "train loss:0.09406728523349069\n",
      "train loss:0.1368473121192774\n",
      "train loss:0.14079868314479793\n",
      "train loss:0.05549297213417801\n",
      "train loss:0.0971014521002977\n",
      "train loss:0.10138692339148837\n",
      "train loss:0.04984696041610711\n",
      "train loss:0.10030299966233175\n",
      "train loss:0.049468948621294515\n",
      "train loss:0.18401081739952713\n",
      "train loss:0.13252823444693917\n",
      "train loss:0.2588006939044528\n",
      "train loss:0.07416136555986709\n",
      "train loss:0.0955879028830449\n",
      "train loss:0.11719382660405801\n",
      "train loss:0.17126838337235564\n",
      "train loss:0.1837791253092134\n",
      "train loss:0.19295170874086268\n",
      "train loss:0.1533153959860869\n",
      "train loss:0.14251833404195485\n",
      "train loss:0.15899777226904968\n",
      "train loss:0.13413555643725653\n",
      "train loss:0.13621593777359892\n",
      "train loss:0.06760444196271019\n",
      "train loss:0.10863747570681839\n",
      "train loss:0.12189151805853696\n",
      "train loss:0.08721691216644616\n",
      "train loss:0.12868328143860228\n",
      "train loss:0.06584585121394923\n",
      "train loss:0.13007817965250004\n",
      "train loss:0.13472172576161923\n",
      "train loss:0.0840377024298518\n",
      "train loss:0.08823131562780136\n",
      "train loss:0.10810873849616273\n",
      "train loss:0.058225197174956486\n",
      "train loss:0.21197702723589507\n",
      "train loss:0.15490549440151524\n",
      "train loss:0.10419367575642337\n",
      "train loss:0.09400084793412428\n",
      "train loss:0.1789817554856793\n",
      "train loss:0.15306418511322853\n",
      "train loss:0.06252663371770865\n",
      "train loss:0.062357625911424845\n",
      "train loss:0.08366689686520026\n",
      "train loss:0.053243532471002906\n",
      "=== epoch:9, train acc:0.959, test acc:0.933 ===\n",
      "train loss:0.10904981981130335\n",
      "train loss:0.173857262673632\n",
      "train loss:0.11253440505031918\n",
      "train loss:0.045963450179559696\n",
      "train loss:0.13793381575040425\n",
      "train loss:0.13993317881255027\n",
      "train loss:0.15469316361406044\n",
      "train loss:0.11983102963040546\n",
      "train loss:0.047401238110911244\n",
      "train loss:0.07862240047658771\n",
      "train loss:0.07644676618162638\n",
      "train loss:0.12002565139433694\n",
      "train loss:0.1692664502570786\n",
      "train loss:0.13409973494443045\n",
      "train loss:0.03992872179620231\n",
      "train loss:0.12137007468026331\n",
      "train loss:0.1048526983464882\n",
      "train loss:0.16617740474881768\n",
      "train loss:0.15225740709944127\n",
      "train loss:0.11466496188763703\n",
      "train loss:0.2209510276686011\n",
      "train loss:0.10573570333552372\n",
      "train loss:0.1327125491208492\n",
      "train loss:0.07087883023189011\n",
      "train loss:0.18028604816534688\n",
      "train loss:0.1355918900511121\n",
      "train loss:0.12380590537465225\n",
      "train loss:0.12901795322624804\n",
      "train loss:0.09524320754405263\n",
      "train loss:0.11253851094415329\n",
      "train loss:0.15496719482410706\n",
      "train loss:0.08992268043276544\n",
      "train loss:0.08414894958845064\n",
      "train loss:0.12376432023221985\n",
      "train loss:0.11832249736793234\n",
      "train loss:0.13896305949909962\n",
      "train loss:0.12388803934290044\n",
      "train loss:0.0702789758667168\n",
      "train loss:0.1435600347685143\n",
      "train loss:0.19110885979900968\n",
      "train loss:0.16619780618702976\n",
      "train loss:0.1303059134017276\n",
      "train loss:0.07547489374552939\n",
      "train loss:0.13453120827731344\n",
      "train loss:0.1256277428081898\n",
      "train loss:0.08249848689467049\n",
      "train loss:0.1212366711614372\n",
      "train loss:0.09187549676420233\n",
      "train loss:0.10372515252312806\n",
      "train loss:0.1777220286201048\n",
      "=== epoch:10, train acc:0.96, test acc:0.927 ===\n",
      "train loss:0.08330511636173993\n",
      "train loss:0.07069522410600364\n",
      "train loss:0.046477769520292596\n",
      "train loss:0.08133574835600449\n",
      "train loss:0.18018326100911466\n",
      "train loss:0.09293776667875658\n",
      "train loss:0.062009267782951405\n",
      "train loss:0.0884856090917748\n",
      "train loss:0.042666926622024565\n",
      "train loss:0.09721361784864704\n",
      "train loss:0.0868837098263232\n",
      "train loss:0.11508910564521646\n",
      "train loss:0.12363398229598623\n",
      "train loss:0.11442197915482336\n",
      "train loss:0.0837664250319831\n",
      "train loss:0.07777780307974562\n",
      "train loss:0.2417779227685957\n",
      "train loss:0.12006878790845221\n",
      "train loss:0.1633205552558355\n",
      "train loss:0.08768331138674941\n",
      "train loss:0.06733103106637851\n",
      "train loss:0.06855411099979816\n",
      "train loss:0.050091492715067944\n",
      "train loss:0.12621309375459172\n",
      "train loss:0.08250594946373256\n",
      "train loss:0.12525218270228855\n",
      "train loss:0.06850352114121311\n",
      "train loss:0.07255938870603952\n",
      "train loss:0.05754844243243824\n",
      "train loss:0.03471230680356118\n",
      "train loss:0.10449538421013205\n",
      "train loss:0.0857879386124245\n",
      "train loss:0.1142007632935018\n",
      "train loss:0.1508498886042428\n",
      "train loss:0.07515577369410609\n",
      "train loss:0.11699965588201186\n",
      "train loss:0.06342318433729859\n",
      "train loss:0.055999298288639025\n",
      "train loss:0.08034033391230565\n",
      "train loss:0.08415270349358533\n",
      "train loss:0.06103362770396558\n",
      "train loss:0.05317757313658047\n",
      "train loss:0.15770260954565848\n",
      "train loss:0.09656433369645745\n",
      "train loss:0.1003171797298438\n",
      "train loss:0.059898978811153206\n",
      "train loss:0.07458624585786533\n",
      "train loss:0.0563162823219524\n",
      "train loss:0.05004764309072557\n",
      "train loss:0.1023718230310058\n",
      "=== epoch:11, train acc:0.969, test acc:0.941 ===\n",
      "train loss:0.13526033773081467\n",
      "train loss:0.05409140195549208\n",
      "train loss:0.052690793468183436\n",
      "train loss:0.09805920950309606\n",
      "train loss:0.05249854598394645\n",
      "train loss:0.06780019992480742\n",
      "train loss:0.13777928608429388\n",
      "train loss:0.059418790074258085\n",
      "train loss:0.12394946988366501\n",
      "train loss:0.16103118198609842\n",
      "train loss:0.06670939421008674\n",
      "train loss:0.1539465822650284\n",
      "train loss:0.07467394602325378\n",
      "train loss:0.12728596800071285\n",
      "train loss:0.06831419418339105\n",
      "train loss:0.08119465284348988\n",
      "train loss:0.1030126379399312\n",
      "train loss:0.14184863717090304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.07911832398077857\n",
      "train loss:0.14010199303923487\n",
      "train loss:0.0594556415628617\n",
      "train loss:0.09463775146651181\n",
      "train loss:0.10730566992854955\n",
      "train loss:0.05381357820990749\n",
      "train loss:0.11178752467000994\n",
      "train loss:0.07248459401990733\n",
      "train loss:0.1492186885141472\n",
      "train loss:0.07479063125151414\n",
      "train loss:0.13916994703976188\n",
      "train loss:0.09310152721109471\n",
      "train loss:0.07743532299936326\n",
      "train loss:0.06960469696405844\n",
      "train loss:0.11079314960408829\n",
      "train loss:0.08662246198390727\n",
      "train loss:0.08917281128111228\n",
      "train loss:0.06054023686854679\n",
      "train loss:0.06550083783975942\n",
      "train loss:0.08941758483519456\n",
      "train loss:0.050299100896318914\n",
      "train loss:0.10016522854479976\n",
      "train loss:0.11640078550284672\n",
      "train loss:0.09422995323057133\n",
      "train loss:0.04973081251453307\n",
      "train loss:0.09866812935690257\n",
      "train loss:0.05775183013512783\n",
      "train loss:0.04413124148623489\n",
      "train loss:0.06841893334459219\n",
      "train loss:0.05396722375618914\n",
      "train loss:0.058144183678582324\n",
      "train loss:0.08006353267350338\n",
      "=== epoch:12, train acc:0.973, test acc:0.938 ===\n",
      "train loss:0.05805467271143781\n",
      "train loss:0.04193797005902708\n",
      "train loss:0.05393022995567145\n",
      "train loss:0.059152509635173114\n",
      "train loss:0.10079810800184279\n",
      "train loss:0.0871891058692544\n",
      "train loss:0.0990685599866923\n",
      "train loss:0.08035109644769435\n",
      "train loss:0.07332558202030498\n",
      "train loss:0.09411738003059894\n",
      "train loss:0.06893068164165811\n",
      "train loss:0.04609038163558323\n",
      "train loss:0.03138204121957748\n",
      "train loss:0.051585706769733085\n",
      "train loss:0.08166952903469951\n",
      "train loss:0.08292499746974609\n",
      "train loss:0.0685430436558289\n",
      "train loss:0.07207484116666026\n",
      "train loss:0.05308427487399611\n",
      "train loss:0.07994722653968346\n",
      "train loss:0.04106376933747495\n",
      "train loss:0.04742806513772896\n",
      "train loss:0.11644148474292457\n",
      "train loss:0.11184884413767772\n",
      "train loss:0.1901471610687171\n",
      "train loss:0.060509782952731656\n",
      "train loss:0.0344370234605263\n",
      "train loss:0.16339032768338801\n",
      "train loss:0.0343856144979672\n",
      "train loss:0.06686317620489494\n",
      "train loss:0.06143056380696346\n",
      "train loss:0.04208719302011839\n",
      "train loss:0.029638677418586785\n",
      "train loss:0.05806920120980327\n",
      "train loss:0.07919704294986228\n",
      "train loss:0.14481882548304376\n",
      "train loss:0.09677220642040953\n",
      "train loss:0.021528010995866483\n",
      "train loss:0.0416777395034107\n",
      "train loss:0.07361395750100039\n",
      "train loss:0.030860343920662604\n",
      "train loss:0.05055964230604187\n",
      "train loss:0.07530078924642551\n",
      "train loss:0.024072031042762274\n",
      "train loss:0.10193318203940363\n",
      "train loss:0.04954646041450511\n",
      "train loss:0.02943701968141049\n",
      "train loss:0.07387657361258491\n",
      "train loss:0.12805896522043367\n",
      "train loss:0.051879034953552824\n",
      "=== epoch:13, train acc:0.972, test acc:0.953 ===\n",
      "train loss:0.03496048475470322\n",
      "train loss:0.053565386173556266\n",
      "train loss:0.0457263758291765\n",
      "train loss:0.036822753932301305\n",
      "train loss:0.06844683624177929\n",
      "train loss:0.022265042778245783\n",
      "train loss:0.023236163656072874\n",
      "train loss:0.04734157237458633\n",
      "train loss:0.06116364813500679\n",
      "train loss:0.04978140608982717\n",
      "train loss:0.029968497005933705\n",
      "train loss:0.14711222284327144\n",
      "train loss:0.06022019660536619\n",
      "train loss:0.060571020298273055\n",
      "train loss:0.08278704108402948\n",
      "train loss:0.08873512882129543\n",
      "train loss:0.022606713897195582\n",
      "train loss:0.03200825923927\n",
      "train loss:0.08333157542254996\n",
      "train loss:0.05856988951164951\n",
      "train loss:0.09424437293605196\n",
      "train loss:0.04215903984084079\n",
      "train loss:0.03098843696963806\n",
      "train loss:0.038592989438663634\n",
      "train loss:0.10070696315618999\n",
      "train loss:0.022750439472383498\n",
      "train loss:0.04770936392625504\n",
      "train loss:0.04577599402333894\n",
      "train loss:0.0645841829945683\n",
      "train loss:0.060258639732035595\n",
      "train loss:0.034212536406610906\n",
      "train loss:0.05683136331153582\n",
      "train loss:0.11048144929731743\n",
      "train loss:0.04457805521861444\n",
      "train loss:0.10710811260959595\n",
      "train loss:0.06588413686606351\n",
      "train loss:0.0751863411854399\n",
      "train loss:0.10802764736240228\n",
      "train loss:0.022324009383736777\n",
      "train loss:0.03656900616573605\n",
      "train loss:0.044443328219299374\n",
      "train loss:0.06650789958042319\n",
      "train loss:0.05980498362063235\n",
      "train loss:0.05557111910166296\n",
      "train loss:0.018173892682256297\n",
      "train loss:0.03417942596935331\n",
      "train loss:0.024293585974725446\n",
      "train loss:0.05106588899232218\n",
      "train loss:0.07566471527636415\n",
      "train loss:0.041672973405849724\n",
      "=== epoch:14, train acc:0.982, test acc:0.944 ===\n",
      "train loss:0.02448449284159358\n",
      "train loss:0.048805344880673174\n",
      "train loss:0.07929760925758671\n",
      "train loss:0.057193657391741023\n",
      "train loss:0.04283055521420362\n",
      "train loss:0.0663730382621288\n",
      "train loss:0.025076460954805965\n",
      "train loss:0.07584159059621891\n",
      "train loss:0.05067135819164864\n",
      "train loss:0.06637533904621555\n",
      "train loss:0.035212745070737676\n",
      "train loss:0.06381485231290919\n",
      "train loss:0.030788160435647968\n",
      "train loss:0.02222259079494698\n",
      "train loss:0.03206544853251523\n",
      "train loss:0.05971486725868894\n",
      "train loss:0.015881418178545707\n",
      "train loss:0.0701877571447622\n",
      "train loss:0.06638452786126187\n",
      "train loss:0.021367307094507307\n",
      "train loss:0.1215164482147129\n",
      "train loss:0.04186712989770897\n",
      "train loss:0.03922506616762696\n",
      "train loss:0.03372101705126442\n",
      "train loss:0.06975519432191646\n",
      "train loss:0.12075729167699858\n",
      "train loss:0.02299197490961056\n",
      "train loss:0.06272828262335826\n",
      "train loss:0.05236582486405302\n",
      "train loss:0.06225290121764324\n",
      "train loss:0.052122505200853195\n",
      "train loss:0.022815016379422584\n",
      "train loss:0.05192935355549411\n",
      "train loss:0.11098459018016013\n",
      "train loss:0.041712103328194944\n",
      "train loss:0.07253450037612347\n",
      "train loss:0.031146304859402067\n",
      "train loss:0.037890102374012906\n",
      "train loss:0.02732882906513379\n",
      "train loss:0.11904387794800889\n",
      "train loss:0.04568708192284319\n",
      "train loss:0.04797071425459136\n",
      "train loss:0.06958077016754365\n",
      "train loss:0.038625943286331677\n",
      "train loss:0.07997401166129742\n",
      "train loss:0.13443854849156125\n",
      "train loss:0.04960937498641064\n",
      "train loss:0.14236438906445004\n",
      "train loss:0.023624032973445883\n",
      "train loss:0.08310044920864619\n",
      "=== epoch:15, train acc:0.983, test acc:0.949 ===\n",
      "train loss:0.06550385681906044\n",
      "train loss:0.1386507279453086\n",
      "train loss:0.019276658301524353\n",
      "train loss:0.1294157673589365\n",
      "train loss:0.06116997857044928\n",
      "train loss:0.030066142940738772\n",
      "train loss:0.102588397235562\n",
      "train loss:0.027795780857719454\n",
      "train loss:0.05026537817970961\n",
      "train loss:0.042079866441837276\n",
      "train loss:0.046960490635819\n",
      "train loss:0.03434248028854302\n",
      "train loss:0.09324504510832954\n",
      "train loss:0.03128477684941086\n",
      "train loss:0.027879381781304086\n",
      "train loss:0.015884019705517404\n",
      "train loss:0.05507544914568875\n",
      "train loss:0.08125013699573058\n",
      "train loss:0.0590951540759172\n",
      "train loss:0.03479965839101056\n",
      "train loss:0.031095634926675962\n",
      "train loss:0.017105022404520574\n",
      "train loss:0.07971903185479354\n",
      "train loss:0.04182808390350818\n",
      "train loss:0.03479067237886026\n",
      "train loss:0.08046841280855924\n",
      "train loss:0.04952479880956157\n",
      "train loss:0.038368626618199037\n",
      "train loss:0.0301475873891862\n",
      "train loss:0.04373796181184171\n",
      "train loss:0.01731622970682508\n",
      "train loss:0.03885378699897489\n",
      "train loss:0.05691094353654747\n",
      "train loss:0.0309862662581736\n",
      "train loss:0.04938301189088324\n",
      "train loss:0.03688264508020909\n",
      "train loss:0.02765270067702344\n",
      "train loss:0.0365783995787094\n",
      "train loss:0.028440359643809122\n",
      "train loss:0.061264149177954\n",
      "train loss:0.02326839082208898\n",
      "train loss:0.05533592138866897\n",
      "train loss:0.060328254780568885\n",
      "train loss:0.050320476968755766\n",
      "train loss:0.03309250134224756\n",
      "train loss:0.024463418800323624\n",
      "train loss:0.0348189460686055\n",
      "train loss:0.005731588022434261\n",
      "train loss:0.025965317340696795\n",
      "train loss:0.03057504966351412\n",
      "=== epoch:16, train acc:0.985, test acc:0.95 ===\n",
      "train loss:0.028532118110020526\n",
      "train loss:0.03948289563998539\n",
      "train loss:0.059391423994842825\n",
      "train loss:0.0190720280989926\n",
      "train loss:0.029294022434883337\n",
      "train loss:0.030813805259111534\n",
      "train loss:0.061284355804136374\n",
      "train loss:0.024029401920768446\n",
      "train loss:0.049254854289182436\n",
      "train loss:0.03947405244057275\n",
      "train loss:0.05926833780203964\n",
      "train loss:0.03418596945601728\n",
      "train loss:0.02419406507189398\n",
      "train loss:0.02396482241309781\n",
      "train loss:0.03598713696983025\n",
      "train loss:0.024589556651787064\n",
      "train loss:0.050160312769849676\n",
      "train loss:0.08788494705242561\n",
      "train loss:0.022893860416461527\n",
      "train loss:0.04646714924110322\n",
      "train loss:0.027887907372757287\n",
      "train loss:0.030631340343882783\n",
      "train loss:0.032019859041082156\n",
      "train loss:0.01958831094366779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0446713830343935\n",
      "train loss:0.07032572794212691\n",
      "train loss:0.031328098505445164\n",
      "train loss:0.01971700897449191\n",
      "train loss:0.03813537468018899\n",
      "train loss:0.01914522029404815\n",
      "train loss:0.04194851727882615\n",
      "train loss:0.03403015690502777\n",
      "train loss:0.043919203307425055\n",
      "train loss:0.02012073038799389\n",
      "train loss:0.02945954475415375\n",
      "train loss:0.06606325631544994\n",
      "train loss:0.01126113671458113\n",
      "train loss:0.017127432789823746\n",
      "train loss:0.06757848679660769\n",
      "train loss:0.008721600955374458\n",
      "train loss:0.04044146583727091\n",
      "train loss:0.022756704486367935\n",
      "train loss:0.03558869591687394\n",
      "train loss:0.04996838132589283\n",
      "train loss:0.021780102444541993\n",
      "train loss:0.02130062127522086\n",
      "train loss:0.044337377685519416\n",
      "train loss:0.032000892198268745\n",
      "train loss:0.014223045637961045\n",
      "train loss:0.022814300101207476\n",
      "=== epoch:17, train acc:0.986, test acc:0.957 ===\n",
      "train loss:0.07585156694047321\n",
      "train loss:0.052910802221372316\n",
      "train loss:0.01331992675321498\n",
      "train loss:0.019898869913831826\n",
      "train loss:0.03135812815222503\n",
      "train loss:0.03026035961076766\n",
      "train loss:0.07287540378149385\n",
      "train loss:0.04442535743591224\n",
      "train loss:0.02441187506867827\n",
      "train loss:0.017489240810012137\n",
      "train loss:0.04250486070301924\n",
      "train loss:0.013360648314985289\n",
      "train loss:0.03021092200652545\n",
      "train loss:0.026612705128074496\n",
      "train loss:0.0443638137255876\n",
      "train loss:0.08491541536245506\n",
      "train loss:0.06882175044513648\n",
      "train loss:0.01477174033273507\n",
      "train loss:0.07570418141641895\n",
      "train loss:0.03270218520915699\n",
      "train loss:0.037777117324320035\n",
      "train loss:0.016497034180231365\n",
      "train loss:0.017059818869855393\n",
      "train loss:0.01802102670671397\n",
      "train loss:0.03255791524543238\n",
      "train loss:0.05821702676975283\n",
      "train loss:0.05069866195005814\n",
      "train loss:0.022203651256302016\n",
      "train loss:0.05487932649290004\n",
      "train loss:0.014286538522041547\n",
      "train loss:0.04228126395235725\n",
      "train loss:0.013573500027078142\n",
      "train loss:0.026867748781062798\n",
      "train loss:0.04015405996693407\n",
      "train loss:0.027477122234965942\n",
      "train loss:0.03601756632974992\n",
      "train loss:0.03718969331259218\n",
      "train loss:0.01808189010434331\n",
      "train loss:0.04000299262813674\n",
      "train loss:0.015260735579736793\n",
      "train loss:0.046440753432042486\n",
      "train loss:0.012680696746990237\n",
      "train loss:0.013948644697017187\n",
      "train loss:0.007232204304964471\n",
      "train loss:0.026007377277167953\n",
      "train loss:0.01474988725192862\n",
      "train loss:0.03492186733678358\n",
      "train loss:0.013979549850106389\n",
      "train loss:0.03600193882113609\n",
      "train loss:0.009149696976386686\n",
      "=== epoch:18, train acc:0.988, test acc:0.955 ===\n",
      "train loss:0.029575868713856715\n",
      "train loss:0.08809972302206223\n",
      "train loss:0.012231989401587672\n",
      "train loss:0.044998765932693086\n",
      "train loss:0.027609739457600403\n",
      "train loss:0.026350194295661405\n",
      "train loss:0.05047479854841359\n",
      "train loss:0.020196237314395495\n",
      "train loss:0.027626329296131274\n",
      "train loss:0.039489898026399774\n",
      "train loss:0.01835122451707816\n",
      "train loss:0.017488867086139413\n",
      "train loss:0.024887125310988058\n",
      "train loss:0.02855689544647979\n",
      "train loss:0.02051071319024146\n",
      "train loss:0.01620847564888366\n",
      "train loss:0.016581195264165693\n",
      "train loss:0.038995662084832246\n",
      "train loss:0.028896878606941293\n",
      "train loss:0.014280143701257257\n",
      "train loss:0.02024368130190239\n",
      "train loss:0.03159505800133748\n",
      "train loss:0.011161056876444104\n",
      "train loss:0.03929328728371746\n",
      "train loss:0.011450663622998116\n",
      "train loss:0.005501544123243927\n",
      "train loss:0.01115682895380673\n",
      "train loss:0.07025186413322872\n",
      "train loss:0.0464204868034928\n",
      "train loss:0.0210765798221986\n",
      "train loss:0.022589418004151117\n",
      "train loss:0.06756715003969993\n",
      "train loss:0.026389342560701835\n",
      "train loss:0.04708728041732859\n",
      "train loss:0.018157135516663855\n",
      "train loss:0.011892978303943784\n",
      "train loss:0.03586951336155689\n",
      "train loss:0.014804937634277808\n",
      "train loss:0.043602137752719754\n",
      "train loss:0.0671523618991054\n",
      "train loss:0.06217154330076308\n",
      "train loss:0.03397310190450609\n",
      "train loss:0.04515763909778547\n",
      "train loss:0.01059863753017256\n",
      "train loss:0.015461988414515777\n",
      "train loss:0.027970442511047234\n",
      "train loss:0.02484630227506007\n",
      "train loss:0.017050154849157072\n",
      "train loss:0.027965670969592946\n",
      "train loss:0.05608581792643826\n",
      "=== epoch:19, train acc:0.988, test acc:0.952 ===\n",
      "train loss:0.011285358046325742\n",
      "train loss:0.013921065106935346\n",
      "train loss:0.015322971743405515\n",
      "train loss:0.021851590414772696\n",
      "train loss:0.04430307276506305\n",
      "train loss:0.012206301739154459\n",
      "train loss:0.03671385474933788\n",
      "train loss:0.01044159235180252\n",
      "train loss:0.03288665437982231\n",
      "train loss:0.013832903171853905\n",
      "train loss:0.013105767101794254\n",
      "train loss:0.0065110508755738305\n",
      "train loss:0.03198351408462313\n",
      "train loss:0.022804981647223138\n",
      "train loss:0.08350494640885207\n",
      "train loss:0.011970279276276754\n",
      "train loss:0.016669941950601258\n",
      "train loss:0.054487439590530186\n",
      "train loss:0.022793753551382145\n",
      "train loss:0.01493900729474375\n",
      "train loss:0.014655153063602842\n",
      "train loss:0.04684930641055672\n",
      "train loss:0.006794130815429269\n",
      "train loss:0.07826496683707225\n",
      "train loss:0.08296376906312218\n",
      "train loss:0.009555138593539536\n",
      "train loss:0.02192584189037372\n",
      "train loss:0.0197602475012185\n",
      "train loss:0.030057780842000156\n",
      "train loss:0.01582817880672759\n",
      "train loss:0.01044989773964675\n",
      "train loss:0.04549484658493935\n",
      "train loss:0.057080382167223165\n",
      "train loss:0.007045921244616333\n",
      "train loss:0.012569624357766221\n",
      "train loss:0.020643323232426302\n",
      "train loss:0.0394981303475732\n",
      "train loss:0.015872091827028926\n",
      "train loss:0.019301742158933554\n",
      "train loss:0.03783098507796622\n",
      "train loss:0.01571429137682525\n",
      "train loss:0.023202366918131438\n",
      "train loss:0.013544724185264971\n",
      "train loss:0.024533265246647947\n",
      "train loss:0.010666338385507521\n",
      "train loss:0.009242464960778154\n",
      "train loss:0.017871507823498223\n",
      "train loss:0.011863679285250927\n",
      "train loss:0.03495228186244868\n",
      "train loss:0.0145564181306494\n",
      "=== epoch:20, train acc:0.996, test acc:0.954 ===\n",
      "train loss:0.013150304020680688\n",
      "train loss:0.008137532655240613\n",
      "train loss:0.008859373968811897\n",
      "train loss:0.02538140893765016\n",
      "train loss:0.019677134864440032\n",
      "train loss:0.013172072790905038\n",
      "train loss:0.061960424403670486\n",
      "train loss:0.024430788509359373\n",
      "train loss:0.008235349765508925\n",
      "train loss:0.007331910260652988\n",
      "train loss:0.01743128269329387\n",
      "train loss:0.007758250004679338\n",
      "train loss:0.020901715822118257\n",
      "train loss:0.02574010398560453\n",
      "train loss:0.012128380839490673\n",
      "train loss:0.006956816200729907\n",
      "train loss:0.018735772781222425\n",
      "train loss:0.02003106010447294\n",
      "train loss:0.013238786201538262\n",
      "train loss:0.008291071530052154\n",
      "train loss:0.0481973045681944\n",
      "train loss:0.008243208227527714\n",
      "train loss:0.012302068097157962\n",
      "train loss:0.028521091805691248\n",
      "train loss:0.006390917773887031\n",
      "train loss:0.020109415248878035\n",
      "train loss:0.023146176953290513\n",
      "train loss:0.020576883593069327\n",
      "train loss:0.01945016989840773\n",
      "train loss:0.017819701319724175\n",
      "train loss:0.019111416122232792\n",
      "train loss:0.021559063403715842\n",
      "train loss:0.017599802497237107\n",
      "train loss:0.013152415184457356\n",
      "train loss:0.016389472107588253\n",
      "train loss:0.02171022815556161\n",
      "train loss:0.013361347988793928\n",
      "train loss:0.009639334512353643\n",
      "train loss:0.050874053480497984\n",
      "train loss:0.025393618125800606\n",
      "train loss:0.05829697063468189\n",
      "train loss:0.0246400665898419\n",
      "train loss:0.04577884459537618\n",
      "train loss:0.012068212811428145\n",
      "train loss:0.0225019707905806\n",
      "train loss:0.023273982399749107\n",
      "train loss:0.033218640323650746\n",
      "train loss:0.024651129882932114\n",
      "train loss:0.018613179787673212\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.958\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#학습 수행\n",
    "#MNIST 데이터 셋 학습하기\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 수행\n",
    "#### 합성곱 계층과 풀링 계층은 이미지 인식에 필수적인 모듈\n",
    "#### 이미지라는 공간적인 형상에 담긴 특징을 CNN이 잘 파악하여 손글씨 숫자 인식에서 높은 정확도를 달성할 수 있음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.6 CNN 시각화 하기\n",
    "## 7.6.1 1번째 층의 가중치 시각화하기\n",
    "MNIST 데이터셋으로 간단한 CNN 학습 수행. 이때 1번째 층의 합성곱 계층의 가중치는 그 형상이 (30, 1, 5, 5)였음(필터 30개, 채널 1개, 5X5 크기). \n",
    "\n",
    "\n",
    "필터의 크기가 5X5이고 채널이 1개라는 것은 이 필터를 1채널의 회색조 이미지로 시각화 할 수 있다는 뜻.\n",
    "합성곱 계층(1층째) 필터를 이미지로(학습 전과 후의 가중치를 비교)\n",
    "\n",
    "학습 후에는 가중치가 규칙성을 띔. 이때, 에지(색상에 바뀐 경계선)와 블롭(국소적으로 덩어리진 영역) 등의 원시적인 정보를 추출할 수 있음\n",
    "\n",
    "<img src=\"refimg/7-25.png\">\n",
    "\n",
    "## 7.6.2 층 깊이에 따른 추출 정보 변화\n",
    "합성곱 계층을 여러 겹 쌓을수록 사물의 ‘의미’를 이해하도록 변화\n",
    "\n",
    "<img src=\"refimg/7-26.png\">\n",
    "\n",
    "\n",
    "<img src=\"refimg/conv.png\">\n",
    "\n",
    "# 7.7 대표적인 CNN\n",
    "## LeNet VS AlexNet\n",
    "### 합성곱 계층과 풀링 계층(정확히는 단순히 ‘원소를 줄이기’만 하는 서브샘플링 계층)을 반복하고, 마지막으로 완전연결 계층을 거치면서 결과를 출력\n",
    "## LeNet\n",
    "<img src=\"refimg/lenet.png\">\n",
    "\n",
    "## AlexNet - 합성곱 계층과 풀링 계층을 거듭하며 마지막으로 완전연결 계층을 거쳐 결과를 출력\n",
    "<img src=\"refimg/alexnet.png\">\n",
    "\n",
    "#### 활성화 함수: 시그모이드 vs ReLU\n",
    "#### 서브샘플링 -> 최대 풀링\n",
    "#### 국소적 정규화\n",
    "#### 드롭아웃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
